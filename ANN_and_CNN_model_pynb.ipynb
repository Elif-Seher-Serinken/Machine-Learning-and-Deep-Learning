{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_and_CNN_model.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMInrnCmtO8aymUH9INxhrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elif-s286/Deep-Learning/blob/main/ANN_and_CNN_model_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN MODEL\n",
        "\n"
      ],
      "metadata": {
        "id": "90n4wdPnQ1Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Layer Model\n"
      ],
      "metadata": {
        "id": "ImR1mYpTyMVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrIK-M7h-NyO",
        "outputId": "cc9c5c39-a0be-4b9a-c67f-1b5b80a65238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sbn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ],
      "metadata": {
        "id": "fvZ34L_N2rT6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ANN/fraud_data.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "HIaR7foo_Kir",
        "outputId": "7bf0b0ab-1869-4483-b6fd-334e53518b55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0      1.176563  0.323798  0.536927  1.047002 -0.368652 -0.728586  0.084678   \n",
              "1      0.681109 -3.934776 -3.801827 -1.147468 -0.735540 -0.501097  1.038865   \n",
              "2      1.140729  0.453484  0.247010  2.383132  0.343287  0.432804  0.093380   \n",
              "3     -1.107073 -3.298902 -0.184092 -1.795744  2.137564 -1.684992 -2.015606   \n",
              "4     -0.314818  0.866839 -0.124577 -0.627638  2.651762  3.428128  0.194637   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "21688 -3.959670  3.297819 -1.079436 -2.290106 -1.405133  2.452586 -4.649235   \n",
              "21689 -1.066503  0.539240  0.735343 -0.506800  0.843980 -1.047877  1.141302   \n",
              "21690 -2.175162 -0.441681  1.883137 -0.267440  1.056972  0.136404  0.113595   \n",
              "21691  0.031406  0.694817  0.083233 -0.797912  0.564318 -0.560787  0.805901   \n",
              "21692 -0.312369  0.944738  1.430605  0.627951  0.317725 -0.180406  0.793108   \n",
              "\n",
              "              V8        V9       V10  ...        V21       V22       V23  \\\n",
              "0      -0.069246 -0.266389  0.155315  ...  -0.109627 -0.341365  0.057845   \n",
              "1      -0.626979 -2.274423  1.527782  ...   0.652202  0.272684 -0.982151   \n",
              "2       0.173310 -0.808999  0.775436  ...  -0.003802  0.058556 -0.121177   \n",
              "3      -0.007181 -0.165760  0.869659  ...   0.130648  0.329445  0.927656   \n",
              "4       0.670674 -0.442658  0.133499  ...  -0.312774 -0.799494 -0.064488   \n",
              "...          ...       ...       ...  ...        ...       ...       ...   \n",
              "21688 -12.365464  0.409493  1.251992  ...  12.617463 -2.969195  1.755050   \n",
              "21689  -0.127448 -0.119221 -1.870265  ...  -0.162535 -0.576352 -0.184969   \n",
              "21690  -0.055983  0.765616 -0.087568  ...  -0.201561  0.397761 -0.855500   \n",
              "21691   0.051453 -0.053817 -0.200190  ...  -0.255891 -0.664635  0.018844   \n",
              "21692  -0.104993 -0.493956  0.344477  ...   0.118417  0.609081 -0.270644   \n",
              "\n",
              "            V24       V25       V26       V27       V28  Amount  Class  \n",
              "0      0.499180  0.415211 -0.581949  0.015472  0.018065    4.67      0  \n",
              "1      0.165900  0.360251  0.195321 -0.256273  0.056501  912.00      0  \n",
              "2     -0.304215  0.645893  0.122600 -0.012115 -0.005945    1.00      0  \n",
              "3     -0.049560 -1.892866 -0.575431  0.266573  0.414184   62.10      0  \n",
              "4      0.953062 -0.429550  0.158225  0.076943 -0.015051    2.67      0  \n",
              "...         ...       ...       ...       ...       ...     ...    ...  \n",
              "21688  0.433324 -0.010827 -0.126613  0.200111 -0.160542   29.95      0  \n",
              "21689 -0.136154  0.760012  0.048105 -0.017475  0.092365   85.66      0  \n",
              "21690 -0.627900  0.590977  0.515065  0.433089 -0.150291  131.10      0  \n",
              "21691 -0.539177 -0.504019  0.155133  0.232846  0.079420    4.49      0  \n",
              "21692  0.004333 -0.114185 -0.287989  0.232375 -0.023563   14.90      0  \n",
              "\n",
              "[21693 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a90ddff-c266-46f4-83a6-08964324ad54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.176563</td>\n",
              "      <td>0.323798</td>\n",
              "      <td>0.536927</td>\n",
              "      <td>1.047002</td>\n",
              "      <td>-0.368652</td>\n",
              "      <td>-0.728586</td>\n",
              "      <td>0.084678</td>\n",
              "      <td>-0.069246</td>\n",
              "      <td>-0.266389</td>\n",
              "      <td>0.155315</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109627</td>\n",
              "      <td>-0.341365</td>\n",
              "      <td>0.057845</td>\n",
              "      <td>0.499180</td>\n",
              "      <td>0.415211</td>\n",
              "      <td>-0.581949</td>\n",
              "      <td>0.015472</td>\n",
              "      <td>0.018065</td>\n",
              "      <td>4.67</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.681109</td>\n",
              "      <td>-3.934776</td>\n",
              "      <td>-3.801827</td>\n",
              "      <td>-1.147468</td>\n",
              "      <td>-0.735540</td>\n",
              "      <td>-0.501097</td>\n",
              "      <td>1.038865</td>\n",
              "      <td>-0.626979</td>\n",
              "      <td>-2.274423</td>\n",
              "      <td>1.527782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.652202</td>\n",
              "      <td>0.272684</td>\n",
              "      <td>-0.982151</td>\n",
              "      <td>0.165900</td>\n",
              "      <td>0.360251</td>\n",
              "      <td>0.195321</td>\n",
              "      <td>-0.256273</td>\n",
              "      <td>0.056501</td>\n",
              "      <td>912.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.140729</td>\n",
              "      <td>0.453484</td>\n",
              "      <td>0.247010</td>\n",
              "      <td>2.383132</td>\n",
              "      <td>0.343287</td>\n",
              "      <td>0.432804</td>\n",
              "      <td>0.093380</td>\n",
              "      <td>0.173310</td>\n",
              "      <td>-0.808999</td>\n",
              "      <td>0.775436</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003802</td>\n",
              "      <td>0.058556</td>\n",
              "      <td>-0.121177</td>\n",
              "      <td>-0.304215</td>\n",
              "      <td>0.645893</td>\n",
              "      <td>0.122600</td>\n",
              "      <td>-0.012115</td>\n",
              "      <td>-0.005945</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.107073</td>\n",
              "      <td>-3.298902</td>\n",
              "      <td>-0.184092</td>\n",
              "      <td>-1.795744</td>\n",
              "      <td>2.137564</td>\n",
              "      <td>-1.684992</td>\n",
              "      <td>-2.015606</td>\n",
              "      <td>-0.007181</td>\n",
              "      <td>-0.165760</td>\n",
              "      <td>0.869659</td>\n",
              "      <td>...</td>\n",
              "      <td>0.130648</td>\n",
              "      <td>0.329445</td>\n",
              "      <td>0.927656</td>\n",
              "      <td>-0.049560</td>\n",
              "      <td>-1.892866</td>\n",
              "      <td>-0.575431</td>\n",
              "      <td>0.266573</td>\n",
              "      <td>0.414184</td>\n",
              "      <td>62.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.314818</td>\n",
              "      <td>0.866839</td>\n",
              "      <td>-0.124577</td>\n",
              "      <td>-0.627638</td>\n",
              "      <td>2.651762</td>\n",
              "      <td>3.428128</td>\n",
              "      <td>0.194637</td>\n",
              "      <td>0.670674</td>\n",
              "      <td>-0.442658</td>\n",
              "      <td>0.133499</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.312774</td>\n",
              "      <td>-0.799494</td>\n",
              "      <td>-0.064488</td>\n",
              "      <td>0.953062</td>\n",
              "      <td>-0.429550</td>\n",
              "      <td>0.158225</td>\n",
              "      <td>0.076943</td>\n",
              "      <td>-0.015051</td>\n",
              "      <td>2.67</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21688</th>\n",
              "      <td>-3.959670</td>\n",
              "      <td>3.297819</td>\n",
              "      <td>-1.079436</td>\n",
              "      <td>-2.290106</td>\n",
              "      <td>-1.405133</td>\n",
              "      <td>2.452586</td>\n",
              "      <td>-4.649235</td>\n",
              "      <td>-12.365464</td>\n",
              "      <td>0.409493</td>\n",
              "      <td>1.251992</td>\n",
              "      <td>...</td>\n",
              "      <td>12.617463</td>\n",
              "      <td>-2.969195</td>\n",
              "      <td>1.755050</td>\n",
              "      <td>0.433324</td>\n",
              "      <td>-0.010827</td>\n",
              "      <td>-0.126613</td>\n",
              "      <td>0.200111</td>\n",
              "      <td>-0.160542</td>\n",
              "      <td>29.95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21689</th>\n",
              "      <td>-1.066503</td>\n",
              "      <td>0.539240</td>\n",
              "      <td>0.735343</td>\n",
              "      <td>-0.506800</td>\n",
              "      <td>0.843980</td>\n",
              "      <td>-1.047877</td>\n",
              "      <td>1.141302</td>\n",
              "      <td>-0.127448</td>\n",
              "      <td>-0.119221</td>\n",
              "      <td>-1.870265</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.162535</td>\n",
              "      <td>-0.576352</td>\n",
              "      <td>-0.184969</td>\n",
              "      <td>-0.136154</td>\n",
              "      <td>0.760012</td>\n",
              "      <td>0.048105</td>\n",
              "      <td>-0.017475</td>\n",
              "      <td>0.092365</td>\n",
              "      <td>85.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21690</th>\n",
              "      <td>-2.175162</td>\n",
              "      <td>-0.441681</td>\n",
              "      <td>1.883137</td>\n",
              "      <td>-0.267440</td>\n",
              "      <td>1.056972</td>\n",
              "      <td>0.136404</td>\n",
              "      <td>0.113595</td>\n",
              "      <td>-0.055983</td>\n",
              "      <td>0.765616</td>\n",
              "      <td>-0.087568</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.201561</td>\n",
              "      <td>0.397761</td>\n",
              "      <td>-0.855500</td>\n",
              "      <td>-0.627900</td>\n",
              "      <td>0.590977</td>\n",
              "      <td>0.515065</td>\n",
              "      <td>0.433089</td>\n",
              "      <td>-0.150291</td>\n",
              "      <td>131.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21691</th>\n",
              "      <td>0.031406</td>\n",
              "      <td>0.694817</td>\n",
              "      <td>0.083233</td>\n",
              "      <td>-0.797912</td>\n",
              "      <td>0.564318</td>\n",
              "      <td>-0.560787</td>\n",
              "      <td>0.805901</td>\n",
              "      <td>0.051453</td>\n",
              "      <td>-0.053817</td>\n",
              "      <td>-0.200190</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.255891</td>\n",
              "      <td>-0.664635</td>\n",
              "      <td>0.018844</td>\n",
              "      <td>-0.539177</td>\n",
              "      <td>-0.504019</td>\n",
              "      <td>0.155133</td>\n",
              "      <td>0.232846</td>\n",
              "      <td>0.079420</td>\n",
              "      <td>4.49</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21692</th>\n",
              "      <td>-0.312369</td>\n",
              "      <td>0.944738</td>\n",
              "      <td>1.430605</td>\n",
              "      <td>0.627951</td>\n",
              "      <td>0.317725</td>\n",
              "      <td>-0.180406</td>\n",
              "      <td>0.793108</td>\n",
              "      <td>-0.104993</td>\n",
              "      <td>-0.493956</td>\n",
              "      <td>0.344477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.118417</td>\n",
              "      <td>0.609081</td>\n",
              "      <td>-0.270644</td>\n",
              "      <td>0.004333</td>\n",
              "      <td>-0.114185</td>\n",
              "      <td>-0.287989</td>\n",
              "      <td>0.232375</td>\n",
              "      <td>-0.023563</td>\n",
              "      <td>14.90</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21693 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a90ddff-c266-46f4-83a6-08964324ad54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a90ddff-c266-46f4-83a6-08964324ad54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a90ddff-c266-46f4-83a6-08964324ad54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "nyTT6azU6mwS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCnCETq3IQ68",
        "outputId": "c365d56a-3816-4066-bae4-8c15ffe5c60f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21693, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6TUB1MdLgo0",
        "outputId": "e2f6517f-f072-459a-a8dc-75b2cbc7d2c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21693,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 0)\n",
        "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T"
      ],
      "metadata": {
        "id": "vIhBJMzqLjZ3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_and_layers_sizes(x_train,y_train):\n",
        "  parameters = {\"W1\" : np.random.randn(3,x_train.shape[0]) * 0.1 ,\n",
        "                \"B1\" : np.zeros((3,1)) , \n",
        "                \"W2\" : np.random.randn(y_train.shape[0] , 3 ) * 0.1 , \n",
        "                \"B2\" : np.zeros((y_train.shape[0] , 1))}\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "EOusF5e8LooU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(x_train, parameters):\n",
        "  Z1 = np.dot(parameters[\"W1\"] ,x_train) + parameters[\"B1\"]\n",
        "  A1 = np.tanh(Z1)\n",
        "  Z2 = np.dot(parameters[\"W2\"] ,A1) + parameters[\"B2\"]\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  cache= {\"Z1\": Z1,\n",
        "          \"A1\": A1,\n",
        "          \"Z2\": Z2,\n",
        "          \"A2\": A2}\n",
        "  return A2, cache"
      ],
      "metadata": {
        "id": "2n8dExLsNa54"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A2 , y, parameters):\n",
        "  logprobs = np.multiply((np.log(A2)),y)\n",
        "  cost = -np.sum(logprobs)/y.shape[0]\n",
        "  return cost"
      ],
      "metadata": {
        "id": "xl8JuNKyOxWo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(parameters,cache , x , y):\n",
        "  dZ2 = cache[\"A2\"]-y\n",
        "  dW2 = np.dot(dZ2, cache[\"A1\"].T)/x.shape[1]\n",
        "  dB2 = np.sum(dZ2, axis=1 , keepdims= True )/x.shape[1]\n",
        "  dZ1 = np.dot(parameters[\"W2\"].T,dZ2)* (1-np.power(cache[\"A1\"],2))\n",
        "  dW1 = np.dot(dZ1, x.T)/x.shape[1]\n",
        "  dB1 = np.sum(dZ1, axis=1 ,keepdims=True)/x.shape[1]\n",
        "  grads = {\"dW1\": dW1,\n",
        "           \"dB1\": dB1,\n",
        "           \"dW2\": dW2,\n",
        "           \"dB2\": dB2}\n",
        "  return grads"
      ],
      "metadata": {
        "id": "ukQUiQT9P1yv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning rate değeri 0.01 alındığı zaman train ve test accuracy değerleri 95 civarında hesaplandı\n",
        "# learning rate değeri 0.1 alındığı zaman ise test ve train accuracy değerleri 98 civarında hesaplandı \n",
        "# learning rate değeri daha da arttırılıp 0.4 alındığında train ve test accuracy değerleri çok düşerek 4 ve 5 civarında hesaplandı \n",
        "# tüm bu değelerden sonra en yüksek doğrulukları hesaplayabilmek için learning rate değerinin doğru seçilmesinin önemli olduğu belirlendi.\n",
        "def update_parameters(parameters , grads , learning_rate = 0.1):\n",
        "  parameters = {\"W1\": parameters[\"W1\"]-learning_rate*grads[\"dW1\"],\n",
        "                \"B1\": parameters[\"B1\"]-learning_rate*grads[\"dB1\"],\n",
        "                \"W2\": parameters[\"W2\"]-learning_rate*grads[\"dW2\"],\n",
        "                \"B2\": parameters[\"B2\"]-learning_rate*grads[\"dB2\"]}\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "S_bVVI90R8M5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(parameters,x_test):\n",
        "\n",
        "  A2, cache =forward_propagation(x_test, parameters)\n",
        "  Y_prediction = np.zeros((1,x_test.shape[1]))\n",
        "\n",
        "  for i in range(A2.shape[1]):\n",
        "    if A2[0,i] <= 0.5 :\n",
        "      Y_prediction[0,i]=0\n",
        "    else:\n",
        "      Y_prediction[0,i] = 1\n",
        "  \n",
        "  return Y_prediction"
      ],
      "metadata": {
        "id": "v819qTyHTK27"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def two_laeyer_neural_network(x_train,y_train,x_test,y_test, num_iterations):\n",
        "  cost_list = []\n",
        "  index_list = []\n",
        "\n",
        "  parameters = initialize_parameters_and_layers_sizes(x_train,y_train)\n",
        "\n",
        "  for i in range(0, num_iterations):\n",
        "    A2, cache = forward_propagation(x_train,parameters)\n",
        "\n",
        "    cost= compute_cost(A2,y_train, parameters)\n",
        "\n",
        "    grads = backward_propagation(parameters, cache , x_train, y_train)\n",
        "\n",
        "    parameters = update_parameters(parameters , grads)\n",
        "\n",
        "    if i % 10 ==0:\n",
        "      cost_list.append(cost)\n",
        "      index_list.append(i)\n",
        "      print(\"Cost after iteration %i : %f\" %(i,cost))\n",
        "\n",
        "  y_prediction_test  = predict(parameters , x_test)\n",
        "  y_prediction_train = predict(parameters, x_train)\n",
        "\n",
        "  print(\"train accuracy : {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
        "  print(\"test accuracy : {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
        "  return parameters\n",
        "parameters = two_laeyer_neural_network(x_train, y_train , x_test , y_test , num_iterations=10)"
      ],
      "metadata": {
        "id": "DkBs54ufUGMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19d237e-a665-4601-abf8-ea83bf602853"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0 : 192.034771\n",
            "train accuracy : 98.91204130555043 %\n",
            "test accuracy : 98.9306784660767 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L-Layer Model"
      ],
      "metadata": {
        "id": "GFwbeJkk8iLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T"
      ],
      "metadata": {
        "id": "U9oWp1dzCbix"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_classifier():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(units = 2 , kernel_initializer='uniform' , activation='relu' , input_dim = x_train.shape[1]))\n",
        "  classifier.add(Dense(units = 2 , kernel_initializer='uniform' , activation='relu' ))\n",
        "  classifier.add(Dense(units = 1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
        "  classifier.compile(optimizer='adam',loss='binary_crossentropy' , metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier , epochs = 100)\n",
        "accuracies = cross_val_score(estimator = classifier , X=x_train , y=y_train , cv=3)\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "print(\"Accuracy mean: \"+ str(mean))\n",
        "print(\"Accuracy variance: \"+ str(variance))"
      ],
      "metadata": {
        "id": "1cyOpOsf8FgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cb4daf-c89d-4643-ad8c-69aac840d6c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.4500 - accuracy: 0.9758\n",
            "Epoch 2/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2666 - accuracy: 0.9837\n",
            "Epoch 3/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9837\n",
            "Epoch 4/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9837\n",
            "Epoch 5/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.9837\n",
            "Epoch 6/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.9837\n",
            "Epoch 7/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9837\n",
            "Epoch 8/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0578 - accuracy: 0.9837\n",
            "Epoch 9/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0534 - accuracy: 0.9837\n",
            "Epoch 10/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.9837\n",
            "Epoch 11/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0482 - accuracy: 0.9837\n",
            "Epoch 12/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.9837\n",
            "Epoch 13/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9837\n",
            "Epoch 14/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9837\n",
            "Epoch 15/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0402 - accuracy: 0.9837\n",
            "Epoch 16/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9837\n",
            "Epoch 17/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0358 - accuracy: 0.9837\n",
            "Epoch 18/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9837\n",
            "Epoch 19/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0326 - accuracy: 0.9837\n",
            "Epoch 20/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0303 - accuracy: 0.9837\n",
            "Epoch 21/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0283 - accuracy: 0.9837\n",
            "Epoch 22/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9837\n",
            "Epoch 23/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0258 - accuracy: 0.9924\n",
            "Epoch 24/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0249 - accuracy: 0.9959\n",
            "Epoch 25/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0241 - accuracy: 0.9957\n",
            "Epoch 26/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0238 - accuracy: 0.9958\n",
            "Epoch 27/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9959\n",
            "Epoch 28/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9958\n",
            "Epoch 29/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0223 - accuracy: 0.9960\n",
            "Epoch 30/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9959\n",
            "Epoch 31/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9959\n",
            "Epoch 32/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9959\n",
            "Epoch 33/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9960\n",
            "Epoch 34/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9960\n",
            "Epoch 35/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9963\n",
            "Epoch 36/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9960\n",
            "Epoch 37/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 0.9960\n",
            "Epoch 38/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9962\n",
            "Epoch 39/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9964\n",
            "Epoch 40/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9959\n",
            "Epoch 41/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9960\n",
            "Epoch 42/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9959\n",
            "Epoch 43/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0187 - accuracy: 0.9960\n",
            "Epoch 44/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9960\n",
            "Epoch 45/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9959\n",
            "Epoch 46/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9964\n",
            "Epoch 47/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9962\n",
            "Epoch 48/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9963\n",
            "Epoch 49/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9963\n",
            "Epoch 50/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9962\n",
            "Epoch 51/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9964\n",
            "Epoch 52/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9965\n",
            "Epoch 53/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9961\n",
            "Epoch 54/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9962\n",
            "Epoch 55/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9965\n",
            "Epoch 56/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9963\n",
            "Epoch 57/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9962\n",
            "Epoch 58/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9961\n",
            "Epoch 59/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9964\n",
            "Epoch 60/100\n",
            "339/339 [==============================] - 1s 4ms/step - loss: 0.0168 - accuracy: 0.9965\n",
            "Epoch 61/100\n",
            "339/339 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9963\n",
            "Epoch 62/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9964\n",
            "Epoch 63/100\n",
            "339/339 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9962\n",
            "Epoch 64/100\n",
            "339/339 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9963\n",
            "Epoch 65/100\n",
            "339/339 [==============================] - 1s 4ms/step - loss: 0.0166 - accuracy: 0.9962\n",
            "Epoch 66/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0164 - accuracy: 0.9961\n",
            "Epoch 67/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9966\n",
            "Epoch 68/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0164 - accuracy: 0.9964\n",
            "Epoch 69/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9963\n",
            "Epoch 70/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9967\n",
            "Epoch 71/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9961\n",
            "Epoch 72/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9963\n",
            "Epoch 73/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9963\n",
            "Epoch 74/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9962\n",
            "Epoch 75/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9962\n",
            "Epoch 76/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9964\n",
            "Epoch 77/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9963\n",
            "Epoch 78/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9962\n",
            "Epoch 79/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9961\n",
            "Epoch 80/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9962\n",
            "Epoch 81/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9959\n",
            "Epoch 82/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9965\n",
            "Epoch 83/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9962\n",
            "Epoch 84/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9963\n",
            "Epoch 85/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9963\n",
            "Epoch 86/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9965\n",
            "Epoch 87/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9961\n",
            "Epoch 88/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9964\n",
            "Epoch 89/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9964\n",
            "Epoch 90/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0164 - accuracy: 0.9964\n",
            "Epoch 91/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9966\n",
            "Epoch 92/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9967\n",
            "Epoch 93/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0161 - accuracy: 0.9966\n",
            "Epoch 94/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9965\n",
            "Epoch 95/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9964\n",
            "Epoch 96/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9966\n",
            "Epoch 97/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9962\n",
            "Epoch 98/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9965\n",
            "Epoch 99/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9966\n",
            "Epoch 100/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9966\n",
            "170/170 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9970\n",
            "Epoch 1/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.9839\n",
            "Epoch 2/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.4827 - accuracy: 0.9956\n",
            "Epoch 3/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.3819 - accuracy: 0.9960\n",
            "Epoch 4/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.3071 - accuracy: 0.9959\n",
            "Epoch 5/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2499 - accuracy: 0.9960\n",
            "Epoch 6/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9960\n",
            "Epoch 7/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9961\n",
            "Epoch 8/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1440 - accuracy: 0.9961\n",
            "Epoch 9/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9960\n",
            "Epoch 10/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1050 - accuracy: 0.9960\n",
            "Epoch 11/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.9960\n",
            "Epoch 12/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.9961\n",
            "Epoch 13/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.9961\n",
            "Epoch 14/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0615 - accuracy: 0.9960\n",
            "Epoch 15/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0547 - accuracy: 0.9961\n",
            "Epoch 16/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0491 - accuracy: 0.9961\n",
            "Epoch 17/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0444 - accuracy: 0.9961\n",
            "Epoch 18/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9962\n",
            "Epoch 19/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9961\n",
            "Epoch 20/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9962\n",
            "Epoch 21/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9964\n",
            "Epoch 22/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9965\n",
            "Epoch 23/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0286 - accuracy: 0.9962\n",
            "Epoch 24/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9962\n",
            "Epoch 25/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0255 - accuracy: 0.9964\n",
            "Epoch 26/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0248 - accuracy: 0.9964\n",
            "Epoch 27/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0237 - accuracy: 0.9965\n",
            "Epoch 28/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0231 - accuracy: 0.9964\n",
            "Epoch 29/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0225 - accuracy: 0.9962\n",
            "Epoch 30/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9967\n",
            "Epoch 31/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9965\n",
            "Epoch 32/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0217 - accuracy: 0.9964\n",
            "Epoch 33/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9966\n",
            "Epoch 34/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9967\n",
            "Epoch 35/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9966\n",
            "Epoch 36/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0226 - accuracy: 0.9962\n",
            "Epoch 37/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9964\n",
            "Epoch 38/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9967\n",
            "Epoch 39/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9966\n",
            "Epoch 40/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9966\n",
            "Epoch 41/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9966\n",
            "Epoch 42/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9964\n",
            "Epoch 43/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9963\n",
            "Epoch 44/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9965\n",
            "Epoch 45/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9964\n",
            "Epoch 46/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0194 - accuracy: 0.9967\n",
            "Epoch 47/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9964\n",
            "Epoch 48/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9965\n",
            "Epoch 49/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9964\n",
            "Epoch 50/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9966\n",
            "Epoch 51/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9966\n",
            "Epoch 52/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9968\n",
            "Epoch 53/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9966\n",
            "Epoch 54/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9964\n",
            "Epoch 55/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9967\n",
            "Epoch 56/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9968\n",
            "Epoch 57/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9967\n",
            "Epoch 58/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9967\n",
            "Epoch 59/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9968\n",
            "Epoch 60/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9964\n",
            "Epoch 61/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9965\n",
            "Epoch 62/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9966\n",
            "Epoch 63/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9967\n",
            "Epoch 64/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9965\n",
            "Epoch 65/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9965\n",
            "Epoch 66/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9965\n",
            "Epoch 67/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9964\n",
            "Epoch 68/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9966\n",
            "Epoch 69/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9967\n",
            "Epoch 70/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9969\n",
            "Epoch 71/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9967\n",
            "Epoch 72/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9964\n",
            "Epoch 73/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9965\n",
            "Epoch 74/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9967\n",
            "Epoch 75/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9968\n",
            "Epoch 76/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9969\n",
            "Epoch 77/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9964\n",
            "Epoch 78/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9968\n",
            "Epoch 79/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9967\n",
            "Epoch 80/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9965\n",
            "Epoch 81/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9967\n",
            "Epoch 82/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9964\n",
            "Epoch 83/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9967\n",
            "Epoch 84/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9968\n",
            "Epoch 85/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9969\n",
            "Epoch 86/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9967\n",
            "Epoch 87/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9964\n",
            "Epoch 88/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9966\n",
            "Epoch 89/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9967\n",
            "Epoch 90/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9968\n",
            "Epoch 91/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9968\n",
            "Epoch 92/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9969\n",
            "Epoch 93/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9966\n",
            "Epoch 94/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9968\n",
            "Epoch 95/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9968\n",
            "Epoch 96/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9969\n",
            "Epoch 97/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0167 - accuracy: 0.9969\n",
            "Epoch 98/100\n",
            "339/339 [==============================] - 1s 1ms/step - loss: 0.0168 - accuracy: 0.9968\n",
            "Epoch 99/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9965\n",
            "Epoch 100/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9966\n",
            "170/170 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 0.9965\n",
            "Epoch 1/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.6162 - accuracy: 0.9874\n",
            "Epoch 2/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.4807 - accuracy: 0.9960\n",
            "Epoch 3/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.3818 - accuracy: 0.9959\n",
            "Epoch 4/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.9962\n",
            "Epoch 5/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2494 - accuracy: 0.9962\n",
            "Epoch 6/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.2059 - accuracy: 0.9958\n",
            "Epoch 7/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9960\n",
            "Epoch 8/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1437 - accuracy: 0.9961\n",
            "Epoch 9/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9959\n",
            "Epoch 10/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.1050 - accuracy: 0.9959\n",
            "Epoch 11/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9961\n",
            "Epoch 12/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.9959\n",
            "Epoch 13/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.9960\n",
            "Epoch 14/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9954\n",
            "Epoch 15/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.9959\n",
            "Epoch 16/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0482 - accuracy: 0.9964\n",
            "Epoch 17/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9965\n",
            "Epoch 18/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.9963\n",
            "Epoch 19/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9961\n",
            "Epoch 20/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9968\n",
            "Epoch 21/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0307 - accuracy: 0.9967\n",
            "Epoch 22/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9968\n",
            "Epoch 23/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9966\n",
            "Epoch 24/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0253 - accuracy: 0.9967\n",
            "Epoch 25/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0261 - accuracy: 0.9963\n",
            "Epoch 26/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0250 - accuracy: 0.9962\n",
            "Epoch 27/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9969\n",
            "Epoch 28/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9967\n",
            "Epoch 29/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9968\n",
            "Epoch 30/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9967\n",
            "Epoch 31/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9967\n",
            "Epoch 32/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9968\n",
            "Epoch 33/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9963\n",
            "Epoch 34/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9969\n",
            "Epoch 35/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9965\n",
            "Epoch 36/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9961\n",
            "Epoch 37/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9969\n",
            "Epoch 38/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9967\n",
            "Epoch 39/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9970\n",
            "Epoch 40/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9967\n",
            "Epoch 41/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9966\n",
            "Epoch 42/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9969\n",
            "Epoch 43/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9966\n",
            "Epoch 44/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9962\n",
            "Epoch 45/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9968\n",
            "Epoch 46/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9969\n",
            "Epoch 47/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9968\n",
            "Epoch 48/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9964\n",
            "Epoch 49/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9969\n",
            "Epoch 50/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9971\n",
            "Epoch 51/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9970\n",
            "Epoch 52/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9969\n",
            "Epoch 53/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9966\n",
            "Epoch 54/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9971\n",
            "Epoch 55/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9968\n",
            "Epoch 56/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9965\n",
            "Epoch 57/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9970\n",
            "Epoch 58/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9969\n",
            "Epoch 59/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0167 - accuracy: 0.9970\n",
            "Epoch 60/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9966\n",
            "Epoch 61/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9967\n",
            "Epoch 62/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9968\n",
            "Epoch 63/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9967\n",
            "Epoch 64/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0170 - accuracy: 0.9970\n",
            "Epoch 65/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9969\n",
            "Epoch 66/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9969\n",
            "Epoch 67/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9963\n",
            "Epoch 68/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9963\n",
            "Epoch 69/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9969\n",
            "Epoch 70/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9967\n",
            "Epoch 71/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9969\n",
            "Epoch 72/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9970\n",
            "Epoch 73/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9968\n",
            "Epoch 74/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9970\n",
            "Epoch 75/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9967\n",
            "Epoch 76/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9963\n",
            "Epoch 77/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9967\n",
            "Epoch 78/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9970\n",
            "Epoch 79/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9969\n",
            "Epoch 80/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9967\n",
            "Epoch 81/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9967\n",
            "Epoch 82/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9970\n",
            "Epoch 83/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9968\n",
            "Epoch 84/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9970\n",
            "Epoch 85/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9968\n",
            "Epoch 86/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9970\n",
            "Epoch 87/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9964\n",
            "Epoch 88/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9968\n",
            "Epoch 89/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9966\n",
            "Epoch 90/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9971\n",
            "Epoch 91/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9968\n",
            "Epoch 92/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9970\n",
            "Epoch 93/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9970\n",
            "Epoch 94/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9967\n",
            "Epoch 95/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9970\n",
            "Epoch 96/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9969\n",
            "Epoch 97/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9970\n",
            "Epoch 98/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9970\n",
            "Epoch 99/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9967\n",
            "Epoch 100/100\n",
            "339/339 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9970\n",
            "170/170 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9961\n",
            "Accuracy mean: 0.996557871500651\n",
            "Accuracy variance: 0.0003789178807311455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN MODEL"
      ],
      "metadata": {
        "id": "cmdC5n_5Qv_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "wad8hPB8X7Ci"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n"
      ],
      "metadata": {
        "id": "QsafKsqdBob0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train images shape: \" ,train_images.shape)\n",
        "print(\"test images shape : \" , test_images.shape)\n",
        "train_labels\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiZD46bWZfdl",
        "outputId": "6abf1822-5ffc-434b-b6d9-f9d586339693"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train images shape:  (50000, 32, 32, 3)\n",
            "test images shape :  (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "train_labels = to_categorical(train_labels, num_classes = 10)"
      ],
      "metadata": {
        "id": "wjL3skG8Z4oX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size = 0.1, random_state=2)\n",
        "print(\"x_train shape\",X_train.shape)\n",
        "print(\"x_test shape\",X_val.shape)\n",
        "print(\"y_train shape\",Y_train.shape)\n",
        "print(\"y_test shape\",Y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l4RnMDXbWlt",
        "outputId": "f433ba63-3e43-4b88-fe0d-698c298c4347"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape (45000, 32, 32, 3)\n",
            "x_test shape (5000, 32, 32, 3)\n",
            "y_train shape (45000, 10)\n",
            "y_test shape (5000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "#\n",
        "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (32,32,3)))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "#\n",
        "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "# fully connected\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "4iiwSj1Pbj1Z"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "model.compile(optimizer='adam',\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6tIJGnOWcOGD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # for better result increase the epochs\n",
        "batch_size = 250"
      ],
      "metadata": {
        "id": "-KWA1bP2cjg_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # dimesion reduction\n",
        "        rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
        "        zoom_range = 0.1, # Randomly zoom image 10%\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
        "        height_shift_range=0.1,  # randomly shift images vertically 10%\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "ryQ3ZxvzdNqN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=train_images.shape[0] // batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "47mRXwMtdQxz",
        "outputId": "e215ee94-8036-4941-9fb2-902b05f79105"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "180/200 [==========================>...] - ETA: 5s - loss: 1.7187 - accuracy: 0.3776WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-fe9395021c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\n\u001b[0;32m----> 2\u001b[0;31m                               epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=train_images.shape[0] // batch_size)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1473, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xq01hmJOdXp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}